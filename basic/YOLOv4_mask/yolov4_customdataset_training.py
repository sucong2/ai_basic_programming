# -*- coding: utf-8 -*-
"""yolov4-customDataset_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MC6C8JHLRb9t-BWyPkbeofHEuGNdBiCG
"""

!nvidia-smi

"""# YOLO v4모델로 colab에서 객체인식(Object detection)

### YOLOv3또는 Yolov4모델을 데이터로 학습 시킬 때는 보통 GPU가 내장된 컴퓨터로 진행을 많이 한다. 그러나, GPU가 내장된 PC가 없다고 딥러닝 모델을 학습하지 못할까? Colab으로도 이와같은 학습이 가능하다.(기왕이면 조금 금액을 지불하더라도 Colab Pro로 진행하는 것을 권장한다.)

**실행 환경**  
**1. colab (Pro)**  
**2. darknet(tensorflow나 pytorch와 같은 딥러닝 프레임워크)**  
**3. YOLO v4 (딥러닝 모델)**


참고로 아래와 같이 진행할 경우 YOLO v3모델도 이와같이 실행할 수 있다.

### 이 내용을 설명하는 관련 참조 글의 링크는 다음과 같다.  
 (https://medium.com/@techzizou007/training-a-custom-detector-for-mask-detection-using-yolov4-darknet-61a659d4868)

### 1. Colab Runtime메뉴에서 Change runtime type을 선택한다. 
### 2. hardware accelator로 GPU를 선택한다.

### 구글 드라이브 마운트
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd ..
from google.colab import drive
drive.mount('/content/drive')

import os 

path = "/content/drive/MyDrive/[22년]실시간스타일변환어플리케이션개발/소스코드/YOLOv4_mask"
#만약에 path로 설정된 폴더가 존재하지 않으면
if not os.path.isdir(path):                                                           
    os.mkdir(path)
    print('mkdir'+path)
else:
    print('이미 폴더가 존재합니다.')

# 아래 PATH는 WEIGHT파일을 저장할 폴더이다.
weight_path = path +"/training"
if not os.path.isdir(weight_path):                                                           
    os.mkdir(weight_path)
    print('mkdir'+weight_path)
else:
    print('이미 폴더가 존재합니다.')

"""### 구글 드라이브의 경로를 간단하게 접근하기 위해 /mydrive로 심볼릭 링크를 설정한다."""

# 리눅스에서 SOFT LINK는 바로가기 아이콘과 같은 폴더를 생성
!ln -s /content/drive/MyDrive/[22년]실시간스타일변환어플리케이션개발/소스코드/ /mydrive

# Commented out IPython magic to ensure Python compatibility.
# 현재 WORKING디렉토리를 아래 폴더로 이동
# %cd /mydrive/YOLOv4_mask

# 현재 내가 작업하는 working dir가 어디인지 확인
# 리눅스 티미널 명령
!pwd

path = '/mydrive/YOLOv4_mask/darknet'
if not os.path.isdir(path):
    # yolo v4의 github에서 나의 구글 드라이브에 소스코드를 복제(다운로드), 
    !git clone https://github.com/AlexeyAB/darknet
    print('git clone is done!')
else:
    print("이미 git으로부터 파일을 다운로드 했습니다")

# Commented out IPython magic to ensure Python compatibility.
# github으로부터 성공적으로 다운로드 하면 darknet이라는 폴더가 생성된다.
# 그리고 darknet폴더로 이동
# %cd /mydrive/YOLOv4_mask/darknet/

# darknet -> yolo v4를 실행하기 위한 딥러닝 프레임워크
!ls

"""### opencv 라이브러리 버전 확인하기

darknet이란?  
YOLO를 위해 YOLO저자가 독자적으로 만든 딥러닝 프레임워크
딥러닝 프레임워크를 만든이유 -> 속도
"""

# 우리 노트북은 ubuntu 20.04를 설치하다보니 opencv 4.x 그래서 환경차이가 있습니다.
# darknet의 original이 opencv 3.2 코딩되어 있다.
# opencv가 3.2이기 때문에 특별한 설정x 
!pkg-config --modversion opencv

"""### Makefile의 내용을 직접 수정합니다."""

# Commented out IPython magic to ensure Python compatibility.
# %cd /mydrive/YOLOv4_mask/

# Commented out IPython magic to ensure Python compatibility.
# GPU가 있으면... Makefile을 아래와 같이 수정
# 리눅스에서 동적라이브러리so(윈도우에서는 DLL)
# %cd darknet/
# !sed -i 's/OPENCV=0/OPENCV=1/' Makefile
# !sed -i 's/GPU=0/GPU=1/' Makefile
# !sed -i 's/CUDNN=0/CUDNN=1/' Makefile
# !sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile
# !sed -i 's/LIBSO=0/LIBSO=1/' Makefile

# 문서 파일의 내용을 화면에 직접 출력
!cat Makefile

import os
# 현재 colab에서 최대로 사용 가능한 cpu core수 확인하기
os.cpu_count()

# '-j'뒤의 숫자는 make시 활용할 cpu 코어 갯수를 의미한다.
!make -j2

# %cd /mydrive/YOLOv4_mask/darknet/

# darknet, libdarknet.so파일이 보이면
# 컴파일이 잘 된 것이다.
!ls

"""## 여기까지 진행하면 YOLO v4를 위한 darknet(딥러닝 프레임워크)이 준비"""

!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights

"""## Colab에서 내 PC의 카메라 연결해서 사용하기"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /mydrive/YOLOv4_mask/darknet

# Commented out IPython magic to ensure Python compatibility.
# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from google.colab.patches import cv2_imshow
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time
import matplotlib.pyplot as plt
# %matplotlib inline


# import darknet functions to perform object detections
from darknet import *
# load in our YOLOv4 architecture network
network, class_names, class_colors = load_network("cfg/yolov4.cfg", "cfg/coco.data", "./yolov4.weights")
width = network_width(network)
height = network_height(network)

# darknet helper function to run detection on image
def darknet_helper(img, width, height):
  darknet_image = make_image(width, height, 3)
  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  img_resized = cv2.resize(img_rgb, (width, height),
                              interpolation=cv2.INTER_LINEAR)

  # get image ratios to convert bounding boxes to proper size
  img_height, img_width, _ = img.shape
  width_ratio = img_width/width
  height_ratio = img_height/height

  # run model on darknet style image to get detections
  copy_image_from_bytes(darknet_image, img_resized.tobytes())
  detections = detect_image(network, class_names, darknet_image)
  free_image(darknet_image)
  return detections, width_ratio, height_ratio

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes  

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;
    
    var pendingResolve = null;
    var shutdown = false;
    
    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }
    
    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }
    
    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);
      
      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);
           
      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);
      
      const instruction = document.createElement('div');
      instruction.innerHTML = 
          '<span style="color: red; font-weight: bold;">' +
          'When finished, click here or on the video to stop this demo</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };
      
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);
      
      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();
      
      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }
            
      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }
      
      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;
      
      return {'create': preShow - preCreate, 
              'show': preCapture - preShow, 
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)
  
def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
bbox = ''
count = 0 
while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    # convert JS response to OpenCV Image
    frame = js_to_image(js_reply["img"])

    # create transparent overlay for bounding box
    bbox_array = np.zeros([480,640,4], dtype=np.uint8)

    # call our darknet helper on video frame
    detections, width_ratio, height_ratio = darknet_helper(frame, width, height)

    # loop through detections and draw them on transparent overlay image
    for label, confidence, bbox in detections:
      left, top, right, bottom = bbox2points(bbox)
      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)
      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)
      bbox_array = cv2.putText(bbox_array, "{} [{:.2f}]".format(label, float(confidence)),
                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                        class_colors[label], 2)

    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(bbox_array)
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes

"""## 사용자 데이터셋을 준비
마스크를 쓴 이미지  
마스크를 쓰지 않은 이미지
"""

# 마스크를 쓴 이미지, 쓰지 않은 이미지를 여기 폴더에 넣기위해
path = '/mydrive/yolov4/darknet/data/customData'
if not os.path.isdir(path):
    os.mkdir(path)
    print(path+'폴더를 생성했습니다')
else:
    print("이미 폴더가 존재합니다")

# path = '/mydrive/yolov4/darknet/data/customData'
# print(len(os.listdir(path)))

# 압축풀기
!unzip /mydrive/yolov4/maskDataset.zip -d data/customData

# Commented out IPython magic to ensure Python compatibility.
# 압축푼 폴더로 이동하여
# %cd /mydrive/yolov4/darknet/data/customData

#파일 압축푼것 확인
!ls

"""### custom dataset을 위해   
### "custom.data", "yolov4-custom.cfg", "custom.names" 파일을 수정하고 yolov4 폴더에 업로드한다.

그리고 process.py파일을 yolov4폴더로 업로드한다.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /mydrive/yolov4/darknet

!cp /mydrive/yolov4/yolov4-custom.cfg cfg

!cp /mydrive/yolov4/custom.names data/
!cp /mydrive/yolov4/custom.data  data/

!cp /mydrive/yolov4/process.py .

"""### 데이터셋 생성하기

### **yolov4-custom.cfg파일 설정**
**[Training]**   
**batch=64**로 설정한다.<br> 그러나, 데이터셋의 **이미지 크기가 클 경우**,<br> batch=32나 16으로 줄여야 할 수도 있다.  <br><br>
**max_batches**는 본인 데이터의 클래스 갯수 * (2000 or 3000)  
tensorflow에서는 epochs에 해당
<br>
(mat_batches는 데이터셋의 특성에 따라 달라질 
수 있다.)
<br><br>
**steps**는 max_batches*0.8,  max_batches*0.9로 설정.
<br><br>
**filters**는 [yolo]의 바로 앞의 있는 filters크기만 수정한다. filters의 계산식은 다음과 같다.
(classes+5)x3, 클래스가 2인경우 (2+5)x3=21이다.  
총 3군데를 수정해 준다.
<br><br>
**classes**는 [yolo]에 있는 classes만 수정해준다.  
총 3군데를 수정해 준다.
<br><br>

**[Inference]**  
테스트를 위해서는 batch=1로 설정한다.<br>

데이터셋을 준비한 뒤 process.py를 실행한다.
process.py파일은 데이터셋을 train과 test의 비율을 9:1로 나누어준다.

#### process.py파일을 실행하면 data폴더안에 train.txt와 test.txt파일을 생성해준다.
"""

# data폴더에 train.txt와 test.txt파일을 만들어준다.
# train.txt파일은 학습에 사용되고,
# test.txt 파일은 validation에 사용된다.
!python process.py

# train.txt, test.txt파일 확인
!ls data/

"""### 생성된 train.txt파일의 내용을 한번 확인해보자"""

!cat data/train.txt

"""### pre-trained yolov4 weight파일을 다운로드 하자."""

# yolo v4를 ms coco데이터셋에 사전학습된 파일 
# 현재 경로 위치가 /content/gdrive/My Drive/Advanced_AI/yolov4/darknet에 다운로드
if not os.path.isfile('yolov4.conv.137'):
    !wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137
else:
    print('이미 파일이 존재합니다')

!pwd

"""### 학습 진행하기"""

!ls -al

# colab은 darknet을 컴파일 했을때는 실행 권한이 있다가
# 다시 이 노트북을 열고 colud공간을 할당 받을때는 이상하게... darknet의 실행권한이 없다.
# darknet파일의 실행권한 설정
# 7  111 -> rwx   110 -> rw-
!chmod 700 darknet

# train your custom detector! (uncomment %%capture below if you run into memory issues or your Colab is crashing)
# %%capture

!./darknet detector train data/custom.data cfg/yolov4-custom.cfg yolov4.conv.137 -dont_show -map

"""## **Colab의 Session을 유지하기 위해서는 아래의 코드를 추가해 주자.**
웹브라우저에서 (Ctrl + Shift + i)키를 입력하면 창이 뜬다.  
console탭으로 이동해서 아래의 코드를 추가 해주자.

```
function ClickConnect(){
console.log("Working"); 
document
  .querySelector('#top-toolbar > colab-connect-button')
  .shadowRoot.querySelector('#connect')
  .click() 
}
setInterval(ClickConnect,60000)
```
"""

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# define helper function imShow
def imShow(path):
  import cv2
  import matplotlib.pyplot as plt
#   %matplotlib inline

  image = cv2.imread(path)
  height, width = image.shape[:2]
  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)

  fig = plt.gcf()
  fig.set_size_inches(18, 10)
  plt.axis("off")
  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
  plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %cd /mydrive/yolov4/darknet/

#only works if the training does not get interrupted
imShow('chart.png')

!pwd

"""# 학습이 중단된 경우 아래 코드 실행

### 학습 도중 colab세션이 끊겼을 때 이어서 학습을 진행하려면 다시 구글 드라이브를 마운트 하고 학습을 진행한다.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd ..
from google.colab import drive
drive.mount('/content/drive')

!ln -s /content/drive/MyDrive/ /mydrive

# %cd /mydrive/yolov4/darknet/

!chmod 700 darknet

"""### 이어서 학습을 진행할 때는  yolov4.conv.137파일 대신, 가장 마지막까지 학습을 했던 weights파일명을 적어준다."""

!./darknet detector train data/custom.data cfg/yolov4-custom.cfg ../training/yolov4-custom_last.weights -dont_show -map

!ls -al

"""## 학습된 weight파일을 테스트해보자

### 학습에 맞춰 설정했던 설정을 테스트 조건으로 변경하자.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd cfg
!sed -i 's/batch=64/batch=1/' yolov4-custom.cfg
!sed -i 's/subdivisions=16/subdivisions=1/' yolov4-custom.cfg
# %cd ..

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd /mydrive/yolov4/darknet/

# weight파일 복사 (구글 드라이브에서 colab서버로 파일을 가져오기)
!cp /mydrive/yolov4/training/yolov4-custom_best.weights weights/

!./darknet detector test data/custom.data cfg/yolov4-custom.cfg weights/yolov4-custom_best.weights /mydrive/Advanced_AI/mask_test_images/image2.jpg -thresh 0.5

# Commented out IPython magic to ensure Python compatibility.
# define helper function imShow
def imShow(path):
  import cv2
  import matplotlib.pyplot as plt
#   %matplotlib inline

  image = cv2.imread(path)
  height, width = image.shape[:2]
  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)

  fig = plt.gcf()
  fig.set_size_inches(18, 10)
  plt.axis("off")
  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
  plt.show()

imShow('predictions.jpg')



"""### PC의 웹캠을 연결하고 웹캡의 영상을 colab으로 전송하여 사진을 촬영하고, 결과를 확인해보자."""

#run detector on images captured by webcam for your custom YOLOv4 trained model
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

!pwd

!./darknet detector test data/custom.data cfg/yolov4-custom.cfg /mydrive/yolov4/training/yolov4-custom_best.weights photo.jpg -thresh 0.5
imShow('predictions.jpg')

# Commented out IPython magic to ensure Python compatibility.
# %cd /mydrive/yolov4/darknet/

"""### 웹캠의 라이브 영상을 사용하여 객체인식하기"""

# Commented out IPython magic to ensure Python compatibility.
# Code from theAIGuysCode Github (https://github.com/theAIGuysCode/YOLOv4-Cloud-Tutorial/blob/master/yolov4_webcam.ipynb)
# Adjusted for my custom YOLOv4 trained weights, config and obj.data files

# import dependencies
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from google.colab.patches import cv2_imshow
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time
import matplotlib.pyplot as plt
# %matplotlib inline


# import darknet functions to perform object detections
from darknet import *
# load in our YOLOv4 architecture network
network, class_names, class_colors = load_network("cfg/yolov4-custom.cfg", "data/custom.data", "/mydrive/yolov4/training/yolov4-custom_best.weights")
width = network_width(network)
height = network_height(network)

# darknet helper function to run detection on image
def darknet_helper(img, width, height):
  darknet_image = make_image(width, height, 3)
  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  img_resized = cv2.resize(img_rgb, (width, height),
                              interpolation=cv2.INTER_LINEAR)

  # get image ratios to convert bounding boxes to proper size
  img_height, img_width, _ = img.shape
  width_ratio = img_width/width
  height_ratio = img_height/height

  # run model on darknet style image to get detections
  copy_image_from_bytes(darknet_image, img_resized.tobytes())
  detections = detect_image(network, class_names, darknet_image)
  free_image(darknet_image)
  return detections, width_ratio, height_ratio

# function to convert the JavaScript object into an OpenCV image
def js_to_image(js_reply):
  """
  Params:
          js_reply: JavaScript object containing image from webcam
  Returns:
          img: OpenCV BGR image
  """
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def bbox_to_bytes(bbox_array):
  """
  Params:
          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
  Returns:
        bytes: Base64 image byte string
  """
  # convert array into PIL image
  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
  iobuf = io.BytesIO()
  # format bbox into png for return
  bbox_PIL.save(iobuf, format='png')
  # format return string
  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return bbox_bytes  

# JavaScript to properly create our live video stream using our webcam as input
def video_stream():
  js = Javascript('''
    var video;
    var div = null;
    var stream;
    var captureCanvas;
    var imgElement;
    var labelElement;
    
    var pendingResolve = null;
    var shutdown = false;
    
    function removeDom() {
       stream.getVideoTracks()[0].stop();
       video.remove();
       div.remove();
       video = null;
       div = null;
       stream = null;
       imgElement = null;
       captureCanvas = null;
       labelElement = null;
    }
    
    function onAnimationFrame() {
      if (!shutdown) {
        window.requestAnimationFrame(onAnimationFrame);
      }
      if (pendingResolve) {
        var result = "";
        if (!shutdown) {
          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
          result = captureCanvas.toDataURL('image/jpeg', 0.8)
        }
        var lp = pendingResolve;
        pendingResolve = null;
        lp(result);
      }
    }
    
    async function createDom() {
      if (div !== null) {
        return stream;
      }

      div = document.createElement('div');
      div.style.border = '2px solid black';
      div.style.padding = '3px';
      div.style.width = '100%';
      div.style.maxWidth = '600px';
      document.body.appendChild(div);
      
      const modelOut = document.createElement('div');
      modelOut.innerHTML = "<span>Status:</span>";
      labelElement = document.createElement('span');
      labelElement.innerText = 'No data';
      labelElement.style.fontWeight = 'bold';
      modelOut.appendChild(labelElement);
      div.appendChild(modelOut);
           
      video = document.createElement('video');
      video.style.display = 'block';
      video.width = div.clientWidth - 6;
      video.setAttribute('playsinline', '');
      video.onclick = () => { shutdown = true; };
      stream = await navigator.mediaDevices.getUserMedia(
          {video: { facingMode: "environment"}});
      div.appendChild(video);

      imgElement = document.createElement('img');
      imgElement.style.position = 'absolute';
      imgElement.style.zIndex = 1;
      imgElement.onclick = () => { shutdown = true; };
      div.appendChild(imgElement);
      
      const instruction = document.createElement('div');
      instruction.innerHTML = 
          '<span style="color: red; font-weight: bold;">' +
          'When finished, click here or on the video to stop this demo</span>';
      div.appendChild(instruction);
      instruction.onclick = () => { shutdown = true; };
      
      video.srcObject = stream;
      await video.play();

      captureCanvas = document.createElement('canvas');
      captureCanvas.width = 640; //video.videoWidth;
      captureCanvas.height = 480; //video.videoHeight;
      window.requestAnimationFrame(onAnimationFrame);
      
      return stream;
    }
    async function stream_frame(label, imgData) {
      if (shutdown) {
        removeDom();
        shutdown = false;
        return '';
      }

      var preCreate = Date.now();
      stream = await createDom();
      
      var preShow = Date.now();
      if (label != "") {
        labelElement.innerHTML = label;
      }
            
      if (imgData != "") {
        var videoRect = video.getClientRects()[0];
        imgElement.style.top = videoRect.top + "px";
        imgElement.style.left = videoRect.left + "px";
        imgElement.style.width = videoRect.width + "px";
        imgElement.style.height = videoRect.height + "px";
        imgElement.src = imgData;
      }
      
      var preCapture = Date.now();
      var result = await new Promise(function(resolve, reject) {
        pendingResolve = resolve;
      });
      shutdown = false;
      
      return {'create': preShow - preCreate, 
              'show': preCapture - preShow, 
              'capture': Date.now() - preCapture,
              'img': result};
    }
    ''')

  display(js)
  
def video_frame(label, bbox):
  data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
  return data

# start streaming video from webcam
video_stream()
# label for video
label_html = 'Capturing...'
# initialze bounding box to empty
bbox = ''
count = 0 
while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    # convert JS response to OpenCV Image
    frame = js_to_image(js_reply["img"])

    # create transparent overlay for bounding box
    bbox_array = np.zeros([480,640,4], dtype=np.uint8)

    # call our darknet helper on video frame
    detections, width_ratio, height_ratio = darknet_helper(frame, width, height)

    # loop through detections and draw them on transparent overlay image
    for label, confidence, bbox in detections:
      left, top, right, bottom = bbox2points(bbox)
      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)
      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)
      bbox_array = cv2.putText(bbox_array, "{} [{:.2f}]".format(label, float(confidence)),
                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,
                        class_colors[label], 2)

    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255
    # convert overlay of bbox into bytes
    bbox_bytes = bbox_to_bytes(bbox_array)
    # update bbox so next frame gets new overlay
    bbox = bbox_bytes

"""참조사이트  
https://github.com/techzizou/yolov4-custom_Training
"""

!pwd

