# -*- coding: utf-8 -*-
"""1_Linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W2vi_CdiYPCcePrZBVZ2iXV5nL-xGNa8
"""

!pip list

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

print(torch.cuda.device(0))

gpu = torch.cuda.current_device()
gpu

torch.cuda.is_available()

torch.manual_seed(123)

# y= 2x + 1 학습에 사용할 데이터 준비 
 x_train = torch.FloatTensor([[1],[2],[3]])
 y_train = torch.FloatTensor([[3],[4],[7]])

x_train, y_train

## 모델 선정 = 가설(H) 설정
## w, b에 대한 정의 
## requires_grad = Treu : 경사하강법 학습시에 값을 업데이트 할것인가? 
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
print(W,b)

y_pred = x_train * W + b

## Loss Function 정의 : MSE 정의
Loss = torch.mean((y_pred - y_train)**2)
print(Loss)

LR = 0.01

## Lr : learning(학습률, 경사하강법의 보폭 )
optimizer = optim.SGD([W,b], lr=LR)

## optimizer를 동작하려면, 가장 먼저 optimizer의 기울기값을 초기화 해야함
optimizer.zero_grad()
# Loss 함수를 오차역전파법으로 기울기 구함
Loss.backward()
# W와 b값을 update (한걸음 이동 한거임) -> 반복문 사용으로 여러번 시도
optimizer.step()

#@title 기본 제목 텍스트

import torch
w = torch.tensor(2.0, requires_grad=True)
epochs = 20 

for epochs in range(epochs + 1):
  z = 2*w
  z.zero_grad()
  # backward함수를 실행하기전에 반드시 zero_grad() 함수를 실행해야함
  z.backward()
  print("수식을 미분한 값: {}".format(w.grad))

epochs = 3000 
for epoch in range(epochs+1):
  # 예측값을 구함
  y_pred = x_train*W+b
  # Loss값을 구함
  Loss = torch.mean((y_pred-y_train)**2)

  #optimizer 적용
  optimizer.zero_grad = True
  Loss.backward()
  optimizer.step()


  # 100번째에서 한번 출력
  if epochs % 100 == 0:
    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))

# 데이터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
# 모델 초기화
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=0.01)

nb_epochs = 2800 # 원하는만큼 경사 하강법을 반복
for epoch in range(nb_epochs + 1):

    # H(x) 계산
    y_pred = x_train * W + b

    # cost 계산
    cost = torch.mean((y_pred - y_train) ** 2)

    # 
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))

